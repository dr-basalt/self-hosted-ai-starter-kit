# OpenWebUI - Assistant IA Personnel

## Vue d'ensemble

Cette configuration ajoute OpenWebUI √† votre kit de d√©marrage AI self-hosted avec les fonctionnalit√©s suivantes :

- **Interface ChatGPT-like** accessible via navigateur
- **Mod√®les LLM** : tinyllama, phi3 (par d√©faut), qwen2.5, llama3.2, gemma2
- **STT/TTS** : Faster Whisper + Coqui TTS pour reconnaissance et synth√®se vocale
- **M√©moire r√©flexive** : Sauvegarde automatique des conversations dans Qdrant
- **PWA** : Application mobile-like installable sur smartphone
- **API** : Endpoints pour int√©gration avec applications externes
- **Webhooks n8n** : Int√©gration avec votre workflow existant

## Installation

### üè† D√©veloppement local

#### 1. Pr√©requis

- Docker et Docker Compose install√©s
- Serveur Ollama accessible sur `http://taz.infra.ori3com.cloud:11434`
- Mod√®les LLM disponibles sur votre serveur Ollama

#### 2. D√©marrage local

```bash
# Pour CPU uniquement
docker compose --profile cpu up -d

# Pour GPU NVIDIA
docker compose --profile gpu-nvidia up -d

# Pour GPU AMD
docker compose --profile gpu-amd up -d
```

### üåê D√©ploiement production avec Tailscale

#### 1. Pr√©requis production

- Serveur Linux avec Docker
- Tailscale install√© et configur√©
- Firewall configur√© (ufw recommand√©)

#### 2. D√©ploiement automatique

```bash
# Script de d√©ploiement complet
chmod +x deploy-production.sh
./deploy-production.sh
```

#### 3. Configuration manuelle

```bash
# 1. Installer Tailscale
curl -fsSL https://pkgs.tailscale.com/stable/ubuntu/jammy.noarmor.gpg | sudo tee /usr/share/keyrings/tailscale-archive-keyring.gpg >/dev/null
curl -fsSL https://pkgs.tailscale.com/stable/ubuntu/jammy.tailscale-keyring.list | sudo tee /etc/apt/sources.list.d/tailscale.list
sudo apt-get update && sudo apt-get install tailscale

# 2. Configurer Tailscale
sudo tailscale up --advertise-tags=tag:openwebui-server

# 3. Configurer le firewall
sudo ufw --force reset
sudo ufw default deny incoming
sudo ufw default allow outgoing
sudo ufw allow in on tailscale0
sudo ufw allow from 100.64.0.0/10 to any port 3000,5001,5678,6333
sudo ufw --force enable

# 4. D√©marrer les services
docker compose -f docker-compose.tailscale.yml --profile cpu up -d
```

### 3. T√©l√©chargement des mod√®les

Les mod√®les suivants seront automatiquement t√©l√©charg√©s :

- `tinyllama:1.1b-chat` - Mod√®le l√©ger pour conversations rapides
- `phi3:3.8b` - **Mod√®le par d√©faut** - √©quilibr√© performance/rapidit√©
- `qwen2.5:3b-instruct` - Mod√®le instruct optimis√© pour les t√¢ches
- `llama3.2:latest` - Derni√®re version de Llama 3.2
- `gemma2:2b` - Mod√®le Gemma 2 l√©ger

## Acc√®s

### üè† D√©veloppement local
- **OpenWebUI** : http://localhost:3000
- **n8n** : http://localhost:5678
- **Qdrant** : http://localhost:6333

### üåê Production (via Tailscale)
Apr√®s d√©ploiement, utilisez l'IP Tailscale de votre serveur :
- **OpenWebUI** : http://[TAILSCALE_IP]:3000
- **n8n** : http://[TAILSCALE_IP]:5678
- **Qdrant** : http://[TAILSCALE_IP]:6333

L'IP Tailscale est affich√©e apr√®s le d√©ploiement ou obtenue avec :
```bash
tailscale ip -4
```

### API Endpoints

#### OpenWebUI API
- `POST /api/v1/chat` - Chat avec le mod√®le LLM
- `GET /api/v1/models` - Liste des mod√®les disponibles

#### M√©moire API (port 5001)
- `POST /memory/save` - Sauvegarder une conversation
- `POST /memory/search` - Rechercher des conversations similaires
- `POST /memory/context` - Obtenir le contexte r√©flexif

#### Webhook n8n
- `POST /webhook/openwebui-chat` - Int√©gration via n8n

## Configuration

### Variables d'environnement

Ajoutez ces variables √† votre fichier `.env` :

```env
# OpenWebUI
OLLAMA_HOST=http://taz.infra.ori3com.cloud:11434
QDRANT_HOST=qdrant
QDRANT_PORT=6333

# Mod√®le par d√©faut
DEFAULT_MODEL=phi3:3.8b
```

### Configuration STT/TTS

Le syst√®me utilise :
- **STT** : Faster Whisper (mod√®le `base`, optimis√© CPU)
- **TTS** : Coqui TTS (mod√®le fran√ßais `tts_models/fr/css10/vits`)

## Utilisation

### Interface Web

1. Ouvrez http://localhost:3000
2. S√©lectionnez votre mod√®le pr√©f√©r√© (phi3 par d√©faut)
3. Commencez √† discuter - toutes les conversations sont sauvegard√©es
4. Utilisez les boutons microphone pour STT/TTS

### API Mobile

Pour votre application mobile existante :

```bash
curl -X POST http://localhost:5678/webhook/openwebui-chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Bonjour, comment allez-vous ?",
    "session_id": "mobile_session_123",
    "model": "phi3:3.8b"
  }'
```

### Int√©gration n8n

Le workflow `openwebui-integration.json` est automatiquement import√© et permet :
- R√©ception de messages via webhook
- Appel d'OpenWebUI avec le mod√®le sp√©cifi√©
- Sauvegarde automatique dans la m√©moire Qdrant
- Retour de la r√©ponse avec m√©tadonn√©es

## PWA (Progressive Web App)

### Installation sur mobile

1. Ouvrez http://localhost:3000 sur votre smartphone
2. Ajoutez √† l'√©cran d'accueil
3. L'ic√¥ne appara√Ætra comme une application native
4. Exp√©rience mobile optimis√©e

### Personnalisation

Pour personnaliser l'apparence :

1. Modifiez `openwebui-pwa.json` pour changer :
   - Nom de l'application
   - Couleurs du th√®me
   - Ic√¥nes

2. Ajoutez vos ic√¥nes dans `/app/open-webui/static/icons/`

## M√©moire et RAG

### Fonctionnalit√©s

- **Sauvegarde automatique** : Chaque conversation est stock√©e dans Qdrant
- **Recherche s√©mantique** : Trouve des conversations similaires
- **Contexte r√©flexif** : Utilise l'historique pour am√©liorer les r√©ponses
- **Embeddings locaux** : Utilise `all-MiniLM-L6-v2` pour les vecteurs

### Structure des donn√©es

```json
{
  "user_message": "Question de l'utilisateur",
  "ai_response": "R√©ponse de l'IA",
  "model_used": "phi3:3.8b",
  "session_id": "session_123",
  "timestamp": "2024-01-01T12:00:00Z",
  "combined_text": "User: Question\nAI: R√©ponse"
}
```

## Monitoring

### Logs

```bash
# Logs OpenWebUI
docker logs openwebui

# Logs m√©moire API
docker logs openwebui -f | grep "memory"

# Logs Qdrant
docker logs qdrant
```

### M√©triques

- **Utilisation m√©moire** : Surveillez via l'API `/memory/stats`
- **Performance** : Logs de temps de r√©ponse dans OpenWebUI
- **Stockage** : Taille de la collection Qdrant

## D√©pannage

### Probl√®mes courants

1. **Mod√®les non trouv√©s** :
   ```bash
   docker exec -it ollama-cpu ollama list
   ```

2. **API m√©moire inaccessible** :
   ```bash
   curl http://localhost:5001/memory/search -X POST -H "Content-Type: application/json" -d '{"query":"test"}'
   ```

3. **STT/TTS ne fonctionne pas** :
   - V√©rifiez que ffmpeg est install√©
   - Testez avec un fichier audio simple

### Logs d√©taill√©s

```bash
# Activer les logs d√©taill√©s
docker compose logs -f openwebui
```

## D√©veloppement

### Ajouter un nouveau mod√®le

1. Ajoutez le mod√®le dans `openwebui-config.yml`
2. Ajoutez le pull dans le docker-compose.yml
3. Red√©marrez les services

### Personnaliser l'interface

1. Clonez le repo OpenWebUI
2. Modifiez les composants React
3. Rebuild l'image Docker

## Support

Pour toute question ou probl√®me :
- Consultez les logs Docker
- V√©rifiez la connectivit√© r√©seau
- Testez les endpoints API individuellement
